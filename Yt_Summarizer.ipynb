{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6589c0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROBLEM STATEMENT : Building a youtube summarizer that will summarize the content of the youtube video for saving time by quicker understanding or summarization about the video in a short time'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUTUBE SUMMARIZER PROJECT\n",
    "\n",
    "'''PROBLEM STATEMENT : Building a youtube summarizer that will summarize the content of the youtube video for saving time by quicker understanding or summarization about the video in a short time'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee23e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube_transcript_api in c:\\users\\veena v\\anaconda3\\lib\\site-packages (0.6.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from youtube_transcript_api) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9eb0175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\veena v\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f6e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required modules and libraries\n",
    "\n",
    "import youtube_transcript_api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929dff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the YouTube link: https://www.youtube.com/watch?v=iCvmsMzlF7o\n"
     ]
    }
   ],
   "source": [
    "link = input(\"Enter the YouTube link: \")\n",
    "unique_id = link.split(\"=\")[-1]\n",
    "sub = YouTubeTranscriptApi.get_transcript(unique_id)  \n",
    "subtitle = \" \".join([x['text'] for x in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018f1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dff6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = subtitle.replace(\"\\n\",\" \")\n",
    "sentences = sent_tokenize(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d368f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "organized_sent = {k:v for v,k in enumerate(sentences)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b310719",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(min_df=2, \n",
    "                                    strip_accents='unicode',\n",
    "                                    max_features=None,\n",
    "                                    lowercase = True,\n",
    "                                    token_pattern=r'w{1,}',\n",
    "                                    ngram_range=(1, 3), \n",
    "                                    use_idf=1,\n",
    "                                    smooth_idf=1,\n",
    "                                    sublinear_tf=1,\n",
    "                                    stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad34696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veena V\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Veena V\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['w'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentence_vectors = tf_idf.fit_transform(sentences)\n",
    "sent_scores = np.array(sentence_vectors.sum(axis=1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1d240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "top_n_sentences = [sentences[index] for index in np.argsort(sent_scores, axis=0)[::-1][:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95e55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the scored sentences with their indexes as in the subtitle\n",
    "mapped_sentences = [(sentence,organized_sent[sentence]) for sentence in top_n_sentences]\n",
    "# Ordering the top-n sentences in their original order\n",
    "mapped_sentences = sorted(mapped_sentences, key = lambda x: x[1])\n",
    "ordered_sentences = [element[0] for element in mapped_sentences]\n",
    "# joining the ordered sentence\n",
    "summary = \" \".join(ordered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eb864b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I was a young researcher, doctoral student, my first year, I had a research professor who said to us, \"Here\\'s the thing, if you cannot measure it, it does not exist.\" Because what we do is we take fat from our butts and put it in our cheeks. We pretend like what we\\'re doing doesn\\'t have a huge impact on other people.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6dd799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\veena v\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: requests in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\veena v\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f8fe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Veena V\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b6ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd026c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "input_tensor = tokenizer.encode( subtitle, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ab866f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,   104,  2368,   229, 17648,    35,    38,   437,    10,\n",
       "         29981,  9338,     4,    38,  5555,  1652,   131,    14,    18,    99,\n",
       "            38,   109,     4,   178,  2085,  1652,    32,    95,   414,    19,\n",
       "            10,  7047,     4,   178,    98,    38,    26,     6,    22,  1185,\n",
       "           216,    99,   116,  2612,   218,    75,    47,    95,   486,   162,\n",
       "            10,  9338,    12,  6462,   859, 12853,  1917,   178,    79,   439,\n",
       "             6,    22, 24017,  2489,     4,   345,    18,   117,   215,   631,\n",
       "            72,    36,   574, 35609,    43,   407,    38,   437,   164,     7,\n",
       "          1067,     7,    47,   452,   480,    52,   214,  1686,    59,  5222,\n",
       "         10518,   480,     8,    98,    38,   236,     7,  1137,   103,  1652,\n",
       "            59,    10,  2125,     9,   127,   557,    14, 16894,  4939,   127,\n",
       "         10518,     8,   269,   888,  1714,     5,   169,    14,    38,   697,\n",
       "             8,   657,     8,   173,     8,  4095,     4,     2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_tensor = model.generate(input_tensor, max_length=160, min_length=120, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "outputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f0eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s><s>Sally Kohn: I'm a qualitative researcher. I collect stories; that's what I do. And maybe stories are just data with a soul. And so I said, \"You know what? Why don't you just call me a researcher-storyteller?\" And she went, \"Ha ha. There's no such thing.\" (Laughter) So I'm going to talk to you today -- we're talking about expanding perception -- and so I want to tell some stories about a piece of my research that fundamentally expanded my perception and really actually changed the way that I live and love and work and parent.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs_tensor[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
